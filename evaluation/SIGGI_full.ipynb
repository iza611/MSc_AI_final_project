{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef90e0dc-4fdf-48e8-b7b0-94396dffa497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Target_0_device_0, Average Speed: 18.25ms\n",
      "Name: Target_1_device_0, Average Speed: 12.71ms\n",
      "Name: Target_2_device_0, Average Speed: 14.85ms\n",
      "Name: Target_3_device_0, Average Speed: 12.51ms\n",
      "Name: Target_4_device_0, Average Speed: 13.04ms\n",
      "Name: Target_5_device_0, Average Speed: 16.89ms\n",
      "Name: Target_6_device_0, Average Speed: 12.87ms\n",
      "Name: Target_0_device_cpu, Average Speed: 2495.70ms\n",
      "Name: Target_1_device_cpu, Average Speed: 2579.06ms\n",
      "Name: Target_2_device_cpu, Average Speed: 2664.09ms\n",
      "Name: Target_3_device_cpu, Average Speed: 2727.76ms\n",
      "Name: Target_4_device_cpu, Average Speed: 2718.65ms\n",
      "Name: Target_5_device_cpu, Average Speed: 2459.30ms\n",
      "Name: Target_6_device_cpu, Average Speed: 2698.42ms\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "file_path = './../Person-Tailored Gesture Classification/results/speed.txt'\n",
    "\n",
    "# Read and parse the file\n",
    "results = {}\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        name, speed_str = line.split(':', 1)\n",
    "        name = name.strip()\n",
    "        speed = float(speed_str.strip())\n",
    "        results[name] = speed\n",
    "\n",
    "# Display the parsed results\n",
    "speed_gpu = []\n",
    "speed_cpu = []\n",
    "for name, speed in results.items():\n",
    "    if name.endswith('0'):\n",
    "        speed_gpu.append(speed)\n",
    "    else:\n",
    "        speed_cpu.append(speed)\n",
    "    print(f\"Name: {name}, Average Speed: {speed:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a1624-0e01-4440-8a4f-876c7fecb37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.44419366972787"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(speed_gpu) / len(speed_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39cd907c-c1dc-4604-8b92-e46055b0364b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2620.4244369552252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(speed_cpu) / len(speed_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c47f587-ba61-4bd8-82b2-dd0fb912eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant No. 0 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0    1.00000 0.966667  0.983051\n",
      "     1    0.84375 0.931034  0.885246\n",
      "     2    0.75000 0.750000  0.750000\n",
      "     3    0.75000 1.000000  0.857143\n",
      "     4    1.00000 0.857143  0.923077\n",
      "     5    1.00000 0.857143  0.923077\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.933333\n",
      "Precision       0.890625\n",
      "   Recall       0.893664\n",
      " F1 Score       0.886932\n",
      "Participant No. 1 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.923077 1.000000  0.960000\n",
      "     1   0.909091 0.869565  0.888889\n",
      "     2   1.000000 0.933333  0.965517\n",
      "     3   1.000000 0.888889  0.941176\n",
      "     4   1.000000 0.600000  0.750000\n",
      "     5   0.750000 0.750000  0.750000\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.925000\n",
      "Precision       0.930361\n",
      "   Recall       0.840298\n",
      " F1 Score       0.875930\n",
      "Participant No. 2 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.909091 1.000000  0.952381\n",
      "     1   0.925926 0.806452  0.862069\n",
      "     2   1.000000 0.875000  0.933333\n",
      "     3   1.000000 0.900000  0.947368\n",
      "     4   0.666667 0.666667  0.666667\n",
      "     5   0.600000 0.600000  0.600000\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.900000\n",
      "Precision       0.850281\n",
      "   Recall       0.808020\n",
      " F1 Score       0.826970\n",
      "Participant No. 3 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.967213 0.983333  0.975207\n",
      "     1   0.774194 0.827586  0.800000\n",
      "     2   0.500000 0.666667  0.571429\n",
      "     3   1.000000 0.636364  0.777778\n",
      "     4   0.777778 0.777778  0.777778\n",
      "     5   1.000000 0.800000  0.888889\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.875000\n",
      "Precision       0.836531\n",
      "   Recall       0.781955\n",
      " F1 Score       0.798513\n",
      "Participant No. 4 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.937500 1.000000  0.967742\n",
      "     1   0.965517 0.903226  0.933333\n",
      "     2   1.000000 1.000000  1.000000\n",
      "     3   0.800000 0.888889  0.842105\n",
      "     4   0.900000 0.750000  0.818182\n",
      "     5   1.000000 0.833333  0.909091\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.933333\n",
      "Precision       0.933836\n",
      "   Recall       0.895908\n",
      " F1 Score       0.911742\n",
      "Participant No. 5 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.952381 1.000000  0.975610\n",
      "     1   0.869565 0.869565  0.869565\n",
      "     2   0.857143 0.857143  0.857143\n",
      "     3   1.000000 0.846154  0.916667\n",
      "     4   1.000000 1.000000  1.000000\n",
      "     5   1.000000 0.833333  0.909091\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.941667\n",
      "Precision       0.946515\n",
      "   Recall       0.901033\n",
      " F1 Score       0.921346\n",
      "Participant No. 6 - evaluation on SIGGI_full\n",
      "\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.982759 0.966102  0.974359\n",
      "     1   0.708333 0.944444  0.809524\n",
      "     2   0.900000 0.750000  0.818182\n",
      "     3   1.000000 1.000000  1.000000\n",
      "     4   1.000000 0.823529  0.903226\n",
      "     5   1.000000 1.000000  1.000000\n",
      "\n",
      "   Metric  Average Score\n",
      " Accuracy       0.924370\n",
      "Precision       0.931849\n",
      "   Recall       0.914013\n",
      " F1 Score       0.917548\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to collect results\n",
    "all_overall_results = []\n",
    "all_class_metrics = []\n",
    "\n",
    "for t in range(7):\n",
    "    print(f'Participant No. {t} - evaluation on SIGGI_full')\n",
    "    \n",
    "    # Load ground truth and predictions JSON files\n",
    "    ground_truth_file = f'./../datasets/SIGGI/full/{t}.json'\n",
    "    predictions_file = f'./../Person-Tailored Gesture Classification/results/gpu/predictions_{t}.json' \n",
    "    \n",
    "    # Load ground truth labels\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "\n",
    "    # Load predictions\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    # Extract ground truth labels\n",
    "    gt_labels = {item['image_id']: item['category_id'] for item in ground_truth['annotations']}\n",
    "\n",
    "    # Extract predicted labels\n",
    "    pred_labels = {item['image_id']: item['category_id'] for item in predictions['predictions']}  \n",
    "\n",
    "    # Ensure the prediction keys match the ground truth keys\n",
    "    image_ids = list(gt_labels.keys())\n",
    "    y_true = [gt_labels[img_id] for img_id in image_ids]\n",
    "    y_pred = [pred_labels.get(img_id, None) for img_id in image_ids]\n",
    "\n",
    "    # Calculate metrics    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Create a DataFrame for overall results\n",
    "    overall_results_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Average Score': [accuracy, precision, recall, f1]\n",
    "    })\n",
    "    \n",
    "    # Append to list\n",
    "    all_overall_results.append(overall_results_df)\n",
    "    \n",
    "    # Get unique class labels\n",
    "    classes = sorted(set(y_true + y_pred))\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=classes, average=None)\n",
    "\n",
    "    # Create a DataFrame for per-class metrics\n",
    "    class_metrics_df = pd.DataFrame({\n",
    "        'Class': classes,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "    \n",
    "    # Append to list\n",
    "    all_class_metrics.append(class_metrics_df)\n",
    "    \n",
    "    # Print the DataFrames\n",
    "    print()\n",
    "    print(class_metrics_df.to_string(index=False))\n",
    "    print()\n",
    "    print(overall_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09781c5a-b843-4210-95d1-81221a759653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Per-Class Metrics:\n",
      " Class  Precision   Recall  F1 Score\n",
      "     0   0.953146 0.988015  0.969764\n",
      "     1   0.856625 0.878839  0.864089\n",
      "     2   0.858163 0.833163  0.842229\n",
      "     3   0.935714 0.880042  0.897462\n",
      "     4   0.906349 0.782160  0.834133\n",
      "     5   0.907143 0.810544  0.854307\n",
      "\n",
      "Aggregated Overall Results:\n",
      "   Metric  Average Score\n",
      " Accuracy       0.918958\n",
      " F1 Score       0.876997\n",
      "Precision       0.902857\n",
      "   Recall       0.862127\n"
     ]
    }
   ],
   "source": [
    "# Aggregate overall results\n",
    "aggregate_overall_results_df = pd.concat(all_overall_results).groupby('Metric').mean().reset_index()\n",
    "\n",
    "# Aggregate per-class metrics\n",
    "all_class_metrics_df = pd.concat(all_class_metrics).groupby('Class').mean().reset_index()\n",
    "\n",
    "# Print aggregated results\n",
    "print(\"Aggregated Per-Class Metrics:\")\n",
    "print(all_class_metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAggregated Overall Results:\")\n",
    "print(aggregate_overall_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410d114-0d99-4567-8fbc-370b4cc087be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract cases where target present & correctly identified\n",
    "# calc. average precision etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
