{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e36ca9-6cf2-4590-b4f9-5e2e819ed3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from modules.target_person_detection import target_person_detection\n",
    "from modules.crop_target_box import crop_target_box\n",
    "from modules.gesture_recognition import gesture_recognition\n",
    "from modules.gesture_synthesis import gesture_synthesis\n",
    "from prep.select_device import select_device\n",
    "from prep.load_model import load_targets_model, load_gestures_model\n",
    "from prep.LoadImages import LoadImages\n",
    "from prep.inference_prep import img_prep, warmup\n",
    "from prep.time_synch import time_synchronized\n",
    "from prep.plots import plot_one_box\n",
    "from prep.label_mapping import class_to_label\n",
    "from prep.save_predictions import covert_to_COCO_and_save_json\n",
    "\n",
    "def main(opt):\n",
    "    target, img_source, show_plots, save = opt.target, opt.img_source, opt.show_plots, opt.save\n",
    "    results = {}\n",
    "    speed = []\n",
    "    \n",
    "    # Initialize\n",
    "    device = select_device('0')\n",
    "    half = device.type != 'cpu' \n",
    "    \n",
    "    # Load all models and prep everything that I'll need for inference\n",
    "    targets_model, stride = load_targets_model(target, device, half)\n",
    "    gestures_model, stride_g = load_gestures_model(device, half)\n",
    "    \n",
    "    # Load DataLoader\n",
    "    dataset = LoadImages(img_source, img_size=1024, stride=stride)\n",
    "    print(dataset)\n",
    "    \n",
    "    # Prep for warmup\n",
    "    old_img_w = old_img_h = 1024\n",
    "    old_img_b = 1\n",
    "    \n",
    "    # Initialize plot\n",
    "    if show_plots:\n",
    "        num_columns = 3\n",
    "        num_rows = (dataset.nf + num_columns - 1) // num_columns\n",
    "        fig, axes = plt.subplots(num_rows, num_columns * 2, figsize=(15, 5 * num_rows))\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        axes = axes.flatten()\n",
    "        index = 0\n",
    "    \n",
    "    for path, img, im0s in tqdm(dataset):\n",
    "        img = img_prep(img, device, half)\n",
    "        warmup(targets_model, device, old_img_b, old_img_h, old_img_w, img)\n",
    "        \n",
    "        # Start timer\n",
    "        t1 = time_synchronized()\n",
    "\n",
    "        # Detect target person on imgs\n",
    "        person_bboxs = target_person_detection(targets_model, path, img, im0s, opt.conf_thres)\n",
    "        \n",
    "        # Extract\n",
    "        person_extracted_img, xyxy = crop_target_box(person_bboxs, im0s)\n",
    "        \n",
    "        if isinstance(person_extracted_img, int) and person_extracted_img == -1:\n",
    "            p = -1\n",
    "            \n",
    "        else:\n",
    "            # Detect gestures\n",
    "            img = img_prep(person_extracted_img, device, half, cropped_img=True, stride=stride_g)\n",
    "            gestures_preds = gesture_recognition(gestures_model, path, img, person_extracted_img, opt.conf_thres)\n",
    "            \n",
    "            # Classify\n",
    "            p = gesture_synthesis(gestures_preds)\n",
    "        \n",
    "        # Stop timer & save results\n",
    "        t2 = time_synchronized()\n",
    "        full_time = 1E3 * (t2 - t1)\n",
    "        speed.append(full_time)\n",
    "        results[os.path.basename(path)] = int(p)\n",
    "        \n",
    "        # Prep subplot\n",
    "        if show_plots:\n",
    "            plot_one_box(xyxy, im0s, color=[0, 255, 0], label='target', line_thickness=8)\n",
    "            for gesture_pred in gestures_preds:\n",
    "                label = str(round(gesture_pred[4], 2)) + \" \" + class_to_label(gesture_pred[5])\n",
    "                plot_one_box(gesture_pred[0:4], person_extracted_img, color=[0, 0, 255], label=label, line_thickness=4)\n",
    "\n",
    "            ax1 = axes[index * 2]\n",
    "            ax1.imshow(cv2.cvtColor(im0s, cv2.COLOR_BGR2RGB))\n",
    "            ax1.set_title(os.path.basename(path))\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = axes[index * 2 + 1]\n",
    "            ax2.imshow(cv2.cvtColor(person_extracted_img, cv2.COLOR_BGR2RGB))\n",
    "            ax2.set_title(f'p={int(p)}')\n",
    "            ax2.axis('off')\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        \n",
    "    # Print final results\n",
    "    print(results)\n",
    "    print(speed)\n",
    "    average_speed = (sum(speed) / len(speed))\n",
    "    print(f\"average time: {average_speed:.1f}ms\")\n",
    "    \n",
    "    if show_plots:\n",
    "        for i in range(index * 2, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    if save:\n",
    "        covert_to_COCO_and_save_json(target, results)\n",
    "        with open('./results/speed.txt', 'a') as f:\n",
    "            f.write(f\"Target_{target}: {average_speed}\\n\") \n",
    "        print(f\"Average inference time saved to ./results/speed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ee4b4c-6a06-45ed-b6cf-7b5acc578dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448397cf-32dc-4a21-9645-266ec06c1d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/yolo/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/jovyan/yolo/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "<prep.LoadImages.LoadImages object at 0x7fc270290190>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'075823f53f.jpg': 0, '185fcae3be.jpg': 2, '236ba0daed.jpg': 0, '2b70b73c73.jpg': 4, '474d3ae1fe.jpg': 0, '4a8512a9e3.jpg': 0, '523fe5470b.jpg': 4, '5cc8062e8e.jpg': 0, '62f5f46069.jpg': 0, '7a1caec0a6.jpg': 1}\n",
      "[817.6727294921875, 454.8792839050293, 400.96259117126465, 448.43077659606934, 455.15942573547363, 631.4597129821777, 459.1958522796631, 384.6151828765869, 20.54619789123535, 354.22778129577637]\n",
      "average time: 442.7ms\n",
      "Saved all predictions in a COCO-compatible JSON file ./results/predictions_0.json\n",
      "Average inference time saved to ./results/speed.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    target=0,\n",
    "    img_source=\"./../datasets/SIGGI/full/0_small\",\n",
    "    conf_thres=0.231,\n",
    "    show_plots=False,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d4aa5-5ed2-45a7-87ef-9ebe83b0a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee3560-3770-4924-9c32-d3142981c846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo)",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
