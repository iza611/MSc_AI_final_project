{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a56121-6b7a-4290-809a-6823e3f5bb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def time_synchronized():\n",
    "    # pytorch-accurate time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e36ca9-6cf2-4590-b4f9-5e2e819ed3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.target_person_detection import target_person_detection\n",
    "from modules.crop_target_box import crop_target_box\n",
    "from modules.gesture_recognition import gesture_recognition\n",
    "from modules.gesture_synthesis import gesture_synthesis\n",
    "from select_device import select_device\n",
    "from load_model import load_targets_model, load_gestures_model\n",
    "from LoadImages import LoadImages\n",
    "from inference_prep import img_prep, warmup\n",
    "\n",
    "def main(opt):\n",
    "    target, img_source, show_plots, save = opt.target, opt.img_source, opt.show_plots, opt.save\n",
    "    results = {}\n",
    "    speed = []\n",
    "    \n",
    "    # Initialize\n",
    "    device = select_device('0')\n",
    "    half = device.type != 'cpu' \n",
    "    \n",
    "    # Load all models and prep everything that I'll need for inference\n",
    "    targets_model, stride = load_targets_model(target, device, half)\n",
    "    gestures_model, stride_g = load_gestures_model(device, half)\n",
    "    \n",
    "    # Load DataLoader\n",
    "    dataset = LoadImages(img_source, img_size=1024, stride=stride)\n",
    "    print(dataset)\n",
    "    \n",
    "    old_img_w = old_img_h = 1024\n",
    "    old_img_b = 1\n",
    "    \n",
    "    for path, img, im0s in tqdm(dataset):\n",
    "        # print(f\"img {path}\")\n",
    "        # print(f\"SHAPE UNCROPPED: {im0s.shape}\")\n",
    "        img = img_prep(img, device, half)\n",
    "        warmup(targets_model, device, old_img_b, old_img_h, old_img_w, img)\n",
    "        # print(f\"SHAPE UNCROPPED: {img.shape}\")\n",
    "        \n",
    "        # Start timer\n",
    "        t1 = time_synchronized()\n",
    "\n",
    "        # Detect target person on imgs\n",
    "        person_bboxs = target_person_detection(targets_model, path, img, im0s, opt.conf_thres)\n",
    "        # print(f\"target detection completed; bboxs: {person_bboxs}\")\n",
    "        # t2 = time_synchronized()\n",
    "        \n",
    "        # Extract\n",
    "        person_extracted_img = crop_target_box(person_bboxs, im0s)\n",
    "        # if not isinstance(person_extracted_img, int):\n",
    "        #     print(f\"target crop completed; img shape: {person_extracted_img.shape}\")\n",
    "        # t3 = time_synchronized()\n",
    "        \n",
    "        if isinstance(person_extracted_img, int) and person_extracted_img == -1:\n",
    "            # print(\"no target on img detected\")\n",
    "            p = -1\n",
    "            # t6 = time_synchronized()\n",
    "        else:\n",
    "            # Detect gestures\n",
    "            # print(f\"SHAPE CROPPED: {person_extracted_img.shape}\")\n",
    "            img = img_prep(person_extracted_img, device, half, cropped_img=True, stride=stride_g)\n",
    "            # print(f\"SHAPE CROPPED: {img.shape}\")\n",
    "            # warmup(gestures_model, device, old_img_b, old_img_h, old_img_w, img)\n",
    "            \n",
    "            gestures_preds = gesture_recognition(gestures_model, path, img, person_extracted_img, opt.conf_thres)\n",
    "            # print(f\"gestures recognition completed; preds: {gestures_preds}\")\n",
    "            # t4 = time_synchronized()\n",
    "            \n",
    "            # Classify\n",
    "            p = gesture_synthesis(gestures_preds)\n",
    "            # print(f\"gestures synthesis completed; p: {p}\")\n",
    "            # t5 = time_synchronized()\n",
    "        \n",
    "        # Calculate speed\n",
    "        # if t5:\n",
    "        #     full_time = 1E3 * (t5 - t1)\n",
    "        #     p_det_time = 1E3 * (t2 - t1)\n",
    "        #     p_extr_time = 1E3 * (t3 - t2)\n",
    "        #     g_det_time = 1E3 * (t4 - t3)\n",
    "        #     g_cls_time = 1E3 * (t5 - t4)\n",
    "        # else:\n",
    "        #     full_time = 1E3 * (t6 - t1)\n",
    "        #     p_det_time = 1E3 * (t2 - t1)\n",
    "        #     p_extr_time = 1E3 * (t6 - t2)\n",
    "        \n",
    "        t2 = time_synchronized()\n",
    "        full_time = 1E3 * (t2 - t1)\n",
    "        speed.append(full_time)\n",
    "        results[path] = p\n",
    "        \n",
    "        \n",
    "    # Print final results\n",
    "    print(results)\n",
    "    print(speed)\n",
    "    \n",
    "#         # Display plots if enabled\n",
    "#         if show_plots:\n",
    "#             # TODO: add plotting where each row: |original | target detected | cut | gesture detected | prediction p |\n",
    "#         # Save results if enabled\n",
    "        \n",
    "#     if(save):\n",
    "#         # TODO: add saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448397cf-32dc-4a21-9645-266ec06c1d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/yolo/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/jovyan/yolo/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "<LoadImages.LoadImages object at 0x7fc5fd7165f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/075823f53f.jpg': 0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/185fcae3be.jpg': 2.0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/236ba0daed.jpg': 0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/2b70b73c73.jpg': 4.0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/474d3ae1fe.jpg': 0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/4a8512a9e3.jpg': 0.0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/523fe5470b.jpg': 4.0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/5cc8062e8e.jpg': 0.0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/62f5f46069.jpg': 0, '/home/jovyan/Person-Tailored Gesture Classification/../datasets/SIGGI/full/0_small/7a1caec0a6.jpg': 1.0}\n",
      "[839.984655380249, 349.76983070373535, 393.51344108581543, 455.7535648345947, 495.6357479095459, 537.4419689178467, 496.4320659637451, 352.6730537414551, 31.354665756225586, 347.3024368286133]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    target=0,\n",
    "    img_source=\"./../datasets/SIGGI/full/0_small\",\n",
    "    conf_thres=0.231,\n",
    "    show_plots=False,\n",
    "    save=False\n",
    ")\n",
    "\n",
    "main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d4aa5-5ed2-45a7-87ef-9ebe83b0a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee3560-3770-4924-9c32-d3142981c846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo)",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
