{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c37ee9ee-77a0-429d-b467-cb086678c7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from os.path import abspath, join, dirname\n",
    "base_path = abspath('./../training/model/yolov7/')\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "\n",
    "img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n",
    "\n",
    "class LoadImages:  \n",
    "    def __init__(self, path, img_size=640, stride=32):\n",
    "        \n",
    "        # path is directing to the json file with labels\n",
    "        image_ids = []\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for image_id, info in data.items():\n",
    "                image_ids.append(image_id)\n",
    "\n",
    "        image_ids_set = set(image_ids)\n",
    "        print(f'found {len(image_ids)} image IDs in {path}')\n",
    "        print(f'ended up with {len(image_ids_set)} IDs after converting to a set')\n",
    "        \n",
    "        images = []\n",
    "        for image_id in image_ids_set:\n",
    "            image_path = join(dirname(dirname(path)), 'like', image_id + '.jpg')\n",
    "            images.append(image_path)\n",
    "        \n",
    "        print(f'list image path length = {len(images)}')\n",
    "        ni = len(images)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.files = images\n",
    "        self.nf = ni  # number of files\n",
    "        self.video_flag = False\n",
    "        self.mode = 'image'\n",
    "        self.cap = None\n",
    "        assert self.nf > 0, f'No images found in {p}. ' \\\n",
    "                            f'Supported formats are:\\nimages: {img_formats}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.nf:\n",
    "            raise StopIteration\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        # Read image\n",
    "        self.count += 1\n",
    "        img0 = cv2.imread(path)  # BGR\n",
    "        assert img0 is not None, 'Image Not Found ' + path\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0, self.cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "065992e9-51fb-4df6-af8c-5a1346e2ac48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hagrid_xywh_to_xyxy(bboxes, img_shape):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from Hagrid format (xywh) to (xyxy).\n",
    "\n",
    "    Args:\n",
    "        bboxes (list of list): List of bounding boxes in the format\n",
    "                               [[top left X, top left Y, width, height], ...].\n",
    "        img_shape (tuple): Shape of the image as (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "        list of list: List of converted bounding boxes in the format\n",
    "                       [[x_min, y_min, x_max, y_max], ...].\n",
    "    \"\"\"\n",
    "    \n",
    "    height, width, _ = img_shape\n",
    "    xyxy_bboxes = []\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Convert normalized values back to pixel values\n",
    "        x_min = x * width\n",
    "        y_min = y * height\n",
    "        x_max = (x + w) * width\n",
    "        y_max = (y + h) * height\n",
    "        \n",
    "        xyxy_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    return xyxy_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6db490f6-4e9f-40e4-9f3f-579fa00c6542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "# import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import sys\n",
    "from os.path import abspath\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the new path to the system path\n",
    "base_path = abspath('./../training/model/yolov7/')\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "# from utils.datasets import LoadImages\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords\n",
    "from utils.torch_utils import select_device, TracedModel\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "\n",
    "def detect(opt):\n",
    "    source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n",
    "\n",
    "    # Initialize\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "    # if trace:\n",
    "    #     model = TracedModel(model, device, opt.img_size)\n",
    "\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Set Dataloader\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Get all gestures bbxs info\n",
    "    hagrid_annotations = {}\n",
    "    with open(os.path.join(source), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Check if images in the label file are in the test set\n",
    "        for image_id, info in data.items():\n",
    "            hagrid_annotations[image_id] = info\n",
    "    print(f'hagrid_annotations dictionary length = {len(hagrid_annotations)}')\n",
    "    \n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    old_img_w = old_img_h = imgsz\n",
    "    old_img_b = 1\n",
    "\n",
    "    for path, img, im0s, vid_cap in tqdm(dataset):\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Warmup\n",
    "        if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "            old_img_b = img.shape[0]\n",
    "            old_img_h = img.shape[2]\n",
    "            old_img_w = img.shape[3]\n",
    "            for i in range(3):\n",
    "                model(img, augment=opt.augment)[0]\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
    "            pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            print(f\"i {i}\")\n",
    "            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path        p.name=img.jpg\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} person{'s' * (n > 1)}, \"  # add to string\n",
    "                    print(s)\n",
    "                \n",
    "                # print(det)\n",
    "                \n",
    "                best_bbox = None\n",
    "                highest_conf = 0\n",
    "\n",
    "                # Iterate through the detections\n",
    "                for *xyxy, conf, _ in det:\n",
    "                    conf = conf.item()  # Convert confidence tensor to float\n",
    "\n",
    "                    # Update if the current confidence is higher than the highest found\n",
    "                    if conf > highest_conf:\n",
    "                        highest_conf = conf\n",
    "                        best_bbox = xyxy\n",
    "                # print(best_bbox)\n",
    "                \n",
    "            else:\n",
    "                print(\"no human detected\")\n",
    "                print(im0.shape)\n",
    "                best_bbox = torch.tensor([0, 0, im0.shape[1], im0.shape[0]], device='cuda:0')\n",
    "                print(best_bbox)\n",
    "        \n",
    "        # Get ground truth gestures bbxs in xyxy format\n",
    "        gestures_info = hagrid_annotations[p.stem]\n",
    "        gestures_bbxs = hagrid_xywh_to_xyxy(gestures_info['bboxes'], im0.shape)\n",
    "        \n",
    "        print(f'Image {p.name}') # p - path to image\n",
    "        print(f'image loaded, shape: {im0.shape}') # im0 - original image\n",
    "        print(f'human detected, bbox: {best_bbox}') # best bbox - human bbx for cropping in xyxy\n",
    "        print(f'ground truth gestures loaded, bboxes: {gestures_bbxs}') # hand_bbxs - hands bbxs groud truth in xyxy\n",
    "        \n",
    "#     plot_one_box(best_bbox, im0, label='Human', color=(255, 0, 0), line_thickness=3)\n",
    "#     for gesture_bbox in gestures_bbxs:\n",
    "#         plot_one_box(gesture_bbox, im0, label='Gesture', color=(0, 255, 0), line_thickness=3)\n",
    "#         # cv2.imshow(str(p), im0)\n",
    "#         # cv2.waitKey(1)  # 1 millisecond\n",
    "        \n",
    "        \n",
    "#     plt.imshow(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "#     plt.pause(0.001)  # Use a small value for quick transitions\n",
    "#     plt.clf() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1c754d68-d7be-40ff-9fb0-31cdc81723eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "found 100 image IDs in ./../datasets/HaGRID_test/val/like.json\n",
      "ended up with 100 IDs after converting to a set\n",
      "list image path length = 100\n",
      "hagrid_annotations dictionary length = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 002cc8a1-7a8b-4b2f-bf0f-7f74548c37b4.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(224., device='cuda:0'), tensor(1266., device='cuda:0'), tensor(1030., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[381.702096, 1482.3213696, 532.6340112, 1697.596128]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0082629f-d111-424f-8a65-ae8314a7f5ec.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(0., device='cuda:0'), tensor(712., device='cuda:0'), tensor(922., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[332.163864, 1002.2755775999999, 582.5210976, 1366.3686911999998]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00c15276-437f-49ef-84ac-80cab0a01f26.jpg\n",
      "image loaded, shape: (640, 480, 3)\n",
      "human detected, bbox: [tensor(4., device='cuda:0'), tensor(4., device='cuda:0'), tensor(478., device='cuda:0'), tensor(638., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[123.9533472, 229.65696640000002, 231.4332336, 388.2690112]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00bc046d-d2a0-4eec-aaaf-aade7b462af2.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(622., device='cuda:0'), tensor(880., device='cuda:0'), tensor(1180., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[781.6355712, 1158.1375296, 879.1195535999999, 1286.3749248000001], [1090.2753360000002, 1549.7993472, 1186.1894448, 1718.8174655999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 000f0e41-9f04-4d07-b328-2be30e3563ef.jpg\n",
      "image loaded, shape: (1080, 1920, 3)\n",
      "human detected, bbox: [tensor(646., device='cuda:0'), tensor(307., device='cuda:0'), tensor(1269., device='cuda:0'), tensor(979., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[1006.6659840000001, 445.83298560000003, 1154.6716416, 654.0506352]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 005762c1-ddc8-4ea3-becd-05f3ec326b03.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(343., device='cuda:0'), tensor(730., device='cuda:0'), tensor(846., device='cuda:0'), tensor(1888., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[347.9302224, 1402.051488, 421.1425872, 1535.5348608], [568.5979968, 934.9836096, 662.0827824, 1074.2397312]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00277cca-b999-4527-bba3-9461fd2eb45b.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(393., device='cuda:0'), tensor(400., device='cuda:0'), tensor(965., device='cuda:0'), tensor(1662., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[542.7054576, 574.5481536, 650.460096, 717.4170432], [893.4419088, 1054.4097408, 971.6060448000001, 1162.4585664]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 007c575a-9c00-4034-9e6b-3ad43ffd1285.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(16., device='cuda:0'), tensor(449., device='cuda:0'), tensor(1433., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[925.2946800000001, 959.5262784, 1298.1508992, 1545.4515456]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00283977-caa9-4259-9629-d60d52352e37.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(16., device='cuda:0'), tensor(663., device='cuda:0'), tensor(1428., device='cuda:0'), tensor(1902., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[399.30461280000003, 835.5189312, 566.24976, 1057.8582144]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0097b35b-dbda-40e3-82c7-70df32d39e8e.jpg\n",
      "image loaded, shape: (885, 1920, 3)\n",
      "human detected, bbox: [tensor(559., device='cuda:0'), tensor(251., device='cuda:0'), tensor(1280., device='cuda:0'), tensor(878., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[832.5265535999999, 423.5158296, 961.2069504, 609.5359265999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 002d9ca5-18d5-4731-965a-37f0ea3ddd71.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(440., device='cuda:0'), tensor(1234., device='cuda:0'), tensor(1038., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[552.9581424, 1390.5638208, 692.8692192, 1587.8487167999997]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00928418-f7a2-41fe-a664-977d7a925c24.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(319., device='cuda:0'), tensor(118., device='cuda:0'), tensor(1027., device='cuda:0'), tensor(718., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[487.2786176, 338.895936, 636.2341504000001, 576.6089472000001]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00ac838c-5078-4af8-96e4-9deea13d2449.jpg\n",
      "image loaded, shape: (885, 1920, 3)\n",
      "human detected, bbox: [tensor(556., device='cuda:0'), tensor(144., device='cuda:0'), tensor(1310., device='cuda:0'), tensor(880., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[765.4828223999999, 268.9676247, 923.501184, 477.2537376]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00bb069d-3f5a-4a97-b1d1-b9f0cd2355ac.jpg\n",
      "image loaded, shape: (1920, 1437, 3)\n",
      "human detected, bbox: [tensor(104., device='cuda:0'), tensor(674., device='cuda:0'), tensor(1329., device='cuda:0'), tensor(1870., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[632.75959875, 1555.9462656, 865.71822147, 1784.7566208], [902.7131541900001, 1022.7254975999999, 1055.53339083, 1280.636544]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0069d29e-4e71-433b-96e1-09657ca3d7eb.jpg\n",
      "image loaded, shape: (1080, 1920, 3)\n",
      "human detected, bbox: [tensor(521., device='cuda:0'), tensor(94., device='cuda:0'), tensor(1408., device='cuda:0'), tensor(1072., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[810.2366016, 349.963632, 976.1693184, 588.1658616]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00bdb0b3-b1d6-4a44-a69a-23d5ca6a1363.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(200., device='cuda:0'), tensor(420., device='cuda:0'), tensor(1300., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[504.4754304, 736.8314688, 661.9629312000001, 968.1914304000002]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 007e3be7-4095-469b-81aa-427c13454d77.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(282., device='cuda:0'), tensor(223., device='cuda:0'), tensor(1248., device='cuda:0'), tensor(1638., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[294.785928, 1253.1305664, 492.767064, 1439.4936192], [808.4094623999999, 424.2133248, 1005.2552447999999, 741.6840768]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 009c5053-dd12-4d34-b394-747addb2624c.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(76., device='cuda:0'), tensor(208., device='cuda:0'), tensor(1312., device='cuda:0'), tensor(1881., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[681.6880368, 745.16304, 955.1661839999999, 1098.1962624]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0022cb1a-e7c1-48aa-b48c-027dca084de9.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(776., device='cuda:0'), tensor(686., device='cuda:0'), tensor(1210., device='cuda:0'), tensor(1439., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[965.1074688000001, 846.2897423999999, 1059.9542016, 974.1931488]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:01, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 00c5b3b6-420d-481d-bd80-123a3c86b6ae.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(303., device='cuda:0'), tensor(1046., device='cuda:0'), tensor(1194., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[715.4166240000001, 1458.4248768, 892.8457920000001, 1729.9729536]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 005cff15-454f-48cd-a16c-38dcedd9c092.jpg\n",
      "image loaded, shape: (1920, 932, 3)\n",
      "human detected, bbox: [tensor(227., device='cuda:0'), tensor(619., device='cuda:0'), tensor(782., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[422.26694384, 750.0123456, 538.21595476, 917.7186624]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0058a6b3-41d2-4500-b6a2-e2b922d3459b.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(446., device='cuda:0'), tensor(98., device='cuda:0'), tensor(1094., device='cuda:0'), tensor(1910., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[472.03388640000003, 998.613792, 576.052992, 1181.8585728], [719.433576, 443.4839232, 853.6458384, 647.2734336]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:01, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 00644ab6-291c-43b2-9308-9f832b25e5af.jpg\n",
      "image loaded, shape: (1088, 1920, 3)\n",
      "human detected, bbox: [tensor(712., device='cuda:0'), tensor(281., device='cuda:0'), tensor(1454., device='cuda:0'), tensor(1084., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[1095.7829376, 433.9949312, 1210.5086975999998, 608.01341568]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 002f1d76-250f-4322-b1c3-4dd4906780d4.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(447., device='cuda:0'), tensor(344., device='cuda:0'), tensor(1106., device='cuda:0'), tensor(1905., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[795.2474016, 643.0511232, 930.6647279999999, 871.5605952]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 004ae8d7-e079-4665-a5cf-0d9b3fbc9398.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(366., device='cuda:0'), tensor(564., device='cuda:0'), tensor(1196., device='cuda:0'), tensor(1916., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[622.4306976, 991.2478272000001, 849.985128, 1265.6981759999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:02, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00d33495-6758-405f-b01a-01bc90111f77.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(249., device='cuda:0'), tensor(673., device='cuda:0'), tensor(1137., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[503.6707008, 883.9590528, 634.4064432, 1112.295456]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0014182f-d756-42b3-896c-c52de76926bb.jpg\n",
      "image loaded, shape: (1920, 886, 3)\n",
      "human detected, bbox: [tensor(160., device='cuda:0'), tensor(353., device='cuda:0'), tensor(825., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[337.17857878, 684.4971072000001, 490.6545732, 885.350496]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00bf711f-e2cc-4cb5-b0ac-573198cd6455.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(98., device='cuda:0'), tensor(685., device='cuda:0'), tensor(1097., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[466.1243568, 1038.5552256, 630.5940576, 1273.4662464000003], [851.7257712, 1681.1600064, 1049.8747824, 1870.1837952]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:02, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00ba83da-684b-4e7c-b24d-704c49878cca.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(421., device='cuda:0'), tensor(226., device='cuda:0'), tensor(1079., device='cuda:0'), tensor(717., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[658.8876288, 385.0527744, 794.696704, 580.2317856]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00bbfad5-cb07-4511-9a9b-c81708a4f35a.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(485., device='cuda:0'), tensor(225., device='cuda:0'), tensor(1629., device='cuda:0'), tensor(1436., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[979.0283903999999, 789.6855024, 1180.7371584, 1069.5068928]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 009f2d36-52cc-4564-8db8-4586a41243f7.jpg\n",
      "image loaded, shape: (1920, 885, 3)\n",
      "human detected, bbox: [tensor(150., device='cuda:0'), tensor(758., device='cuda:0'), tensor(716., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[151.57923345, 1500.8573376, 238.26894825000002, 1619.3573376], [405.63259035000004, 1005.6997248, 503.79764880000005, 1159.16976]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:02, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 0035dfca-3c94-49f4-93fb-df03dc1514ae.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(396., device='cuda:0'), tensor(748., device='cuda:0'), tensor(1039., device='cuda:0'), tensor(1870., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[559.7073072000001, 1233.4553088, 668.201256, 1346.4320256], [664.3968048, 974.6405952, 765.256896, 1096.68672]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0091d4ef-861c-415a-85eb-b238c217d771.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(138., device='cuda:0'), tensor(590., device='cuda:0'), tensor(1440., device='cuda:0'), tensor(1902., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[506.5164144, 1014.2867136, 767.6384975999999, 1381.4408448]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 004f1c70-1735-49e9-ad16-4b176d555d89.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(532., device='cuda:0'), tensor(1192., device='cuda:0'), tensor(1074., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[794.566944, 1307.4450047999999, 906.3859104000001, 1441.9409856]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:02, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 00569fde-a9fb-4ed4-ae4d-583242e31fa9.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(513., device='cuda:0'), tensor(387., device='cuda:0'), tensor(910., device='cuda:0'), tensor(1728., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[701.02332, 664.8698688, 792.7378127999999, 803.0334144], [845.8509888, 1009.5675648000001, 902.9325167999999, 1112.7648768000001]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 003be7e7-0725-4e19-b81b-2ffaf11b33b7.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(11., device='cuda:0'), tensor(599., device='cuda:0'), tensor(1250., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[318.2838912, 1202.665056, 707.4460367999999, 1713.6924672]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 006d1874-d4ca-4ec1-b2e2-2e3ee7f7f64c.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(330., device='cuda:0'), tensor(630., device='cuda:0'), tensor(1158., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[709.1033616, 979.1399040000001, 896.1187536, 1216.4339328]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00ba514a-8b8c-4d41-a075-c54df8a572f7.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(226., device='cuda:0'), tensor(121., device='cuda:0'), tensor(844., device='cuda:0'), tensor(713., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[533.1026304000001, 411.24474, 649.5965312, 578.0740248]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00439068-26fe-4753-a9df-63b32d993e26.jpg\n",
      "image loaded, shape: (1920, 1442, 3)\n",
      "human detected, bbox: [tensor(25., device='cuda:0'), tensor(12., device='cuda:0'), tensor(1316., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[306.61320983999997, 997.5030144, 615.1452458199999, 1424.9681472]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:03, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00bdebb7-70c9-4a49-b6a4-a904b7ca8eb5.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(148., device='cuda:0'), tensor(202., device='cuda:0'), tensor(1410., device='cuda:0'), tensor(1908., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[725.0528448, 814.5486143999999, 1053.0614736000002, 1239.6759935999999]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 004b66dd-19e0-47cd-968e-c1b56f9e9179.jpg\n",
      "image loaded, shape: (480, 640, 3)\n",
      "human detected, bbox: [tensor(118., device='cuda:0'), tensor(20., device='cuda:0'), tensor(500., device='cuda:0'), tensor(479., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[244.685056, 172.5472128, 303.1488192, 266.2874064]]\n",
      "i 0\n",
      "3 persons, \n",
      "Image 005621d3-5e1f-4833-9abe-a2dfbd8216a6.jpg\n",
      "image loaded, shape: (480, 640, 3)\n",
      "human detected, bbox: [tensor(168., device='cuda:0'), tensor(205., device='cuda:0'), tensor(315., device='cuda:0'), tensor(479., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[319.18197760000004, 365.5793904, 352.5740352, 411.8859168]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 002d5a40-02ca-42a5-a096-53a039ee1895.jpg\n",
      "image loaded, shape: (1920, 1425, 3)\n",
      "human detected, bbox: [tensor(492., device='cuda:0'), tensor(608., device='cuda:0'), tensor(1174., device='cuda:0'), tensor(1833., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[919.14479325, 756.5404224, 1035.15558225, 922.7377536]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0076c52a-b66a-4413-8104-e9d24095b868.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(152., device='cuda:0'), tensor(796., device='cuda:0'), tensor(1239., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[665.3589407999999, 1144.0710912000002, 865.488816, 1415.0170368000001]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:03, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 0055ba40-8dd6-4227-bd60-b26b2367db1d.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(191., device='cuda:0'), tensor(618., device='cuda:0'), tensor(896., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[320.95314720000005, 951.1572288, 481.6436112, 1191.8189568], [368.43141599999996, 1405.7592960000002, 530.3260655999999, 1634.5139904000002]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0077139f-ca88-4a68-bf1d-6cdca0d25144.jpg\n",
      "image loaded, shape: (1200, 1600, 3)\n",
      "human detected, bbox: [tensor(493., device='cuda:0'), tensor(278., device='cuda:0'), tensor(1212., device='cuda:0'), tensor(1200., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[875.892432, 584.846376, 1002.316096, 766.8444719999999]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00d3d9f2-3f88-4061-b066-7db3fd65efec.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(53., device='cuda:0'), tensor(270., device='cuda:0'), tensor(1316., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[280.2854016, 805.2705984, 576.9752256, 1236.3432384]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 001adb45-7b9d-4c6a-8f13-5c311dfe640d.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(30., device='cuda:0'), tensor(1053., device='cuda:0'), tensor(1335., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[549.8271936, 1386.6909312, 772.3989504, 1727.2021055999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:03, 18.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 00aeb6ef-475b-4ff9-b706-3076f5125ea2.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(381., device='cuda:0'), tensor(187., device='cuda:0'), tensor(952., device='cuda:0'), tensor(717., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[605.8656, 441.5113224, 724.2945792, 609.2530056]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00665c3e-a39b-4c9d-bc38-2a09476c7f83.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(8., device='cuda:0'), tensor(10., device='cuda:0'), tensor(1436., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[26.392204800000002, 773.3356224, 613.5726096, 1780.8293952]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00a00d26-2ccb-4827-be61-05c880ec612c.jpg\n",
      "image loaded, shape: (1920, 1434, 3)\n",
      "human detected, bbox: [tensor(441., device='cuda:0'), tensor(228., device='cuda:0'), tensor(1043., device='cuda:0'), tensor(1755., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[628.5531744, 689.8607615999999, 772.7906734200001, 877.7014272]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 005dbbcd-b8de-44fd-be7d-0e64e3f32ae6.jpg\n",
      "image loaded, shape: (1920, 908, 3)\n",
      "human detected, bbox: [tensor(246., device='cuda:0'), tensor(46., device='cuda:0'), tensor(779., device='cuda:0'), tensor(1303., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[248.39755572, 673.91184, 415.46450724, 891.6621888], [372.41983200000004, 415.2217728, 578.82677336, 631.7992896000001]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:03, 18.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00706134-abfb-4b54-8795-f4add9ac6eaf.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(33., device='cuda:0'), tensor(239., device='cuda:0'), tensor(1435., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[475.859736, 875.0003519999999, 750.6302544, 1310.6259072]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0079bfd0-b778-4cc1-9041-0e3f9e8c60f8.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(422., device='cuda:0'), tensor(266., device='cuda:0'), tensor(1382., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[409.73837760000004, 1243.8501311999999, 546.0495552, 1415.9732736], [924.0881327999999, 409.88238720000004, 1073.6085024, 633.2989056]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00325ded-ad36-4df3-8f05-c68cd7d274de.jpg\n",
      "image loaded, shape: (1920, 1080, 3)\n",
      "human detected, bbox: [tensor(350., device='cuda:0'), tensor(819., device='cuda:0'), tensor(950., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[563.8801500000001, 1070.0414208000002, 677.6522424, 1217.8104768], [857.4040188, 1556.4959807999999, 950.017086, 1688.6928]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 001498fe-c99b-473d-a1b1-777aa1538c64.jpg\n",
      "image loaded, shape: (720, 960, 3)\n",
      "human detected, bbox: [tensor(10., device='cuda:0'), tensor(170., device='cuda:0'), tensor(889., device='cuda:0'), tensor(720., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[427.9023744, 427.5240408, 575.2026048, 631.8028368]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:03, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 0036c112-1ede-4c28-9096-74e050a5bf95.jpg\n",
      "image loaded, shape: (1920, 1920, 3)\n",
      "human detected, bbox: [tensor(312., device='cuda:0'), tensor(902., device='cuda:0'), tensor(1272., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[908.6524608, 915.4513728, 1088.51232, 1154.9406528]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 003a02c6-6c42-48cb-b013-7c2d12dea782.jpg\n",
      "image loaded, shape: (1024, 1280, 3)\n",
      "human detected, bbox: [tensor(209., device='cuda:0'), tensor(122., device='cuda:0'), tensor(1267., device='cuda:0'), tensor(1020., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[736.4281344, 543.52603136, 950.717504, 867.08103168]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 003ae419-fa37-409e-8b7c-bfc7e9cebd2b.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(283., device='cuda:0'), tensor(618., device='cuda:0'), tensor(1167., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[338.2033968, 1190.4145728, 524.6124623999999, 1487.7894912000002]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 00905dbe-2b98-4a42-8156-05a5ed766922.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(680., device='cuda:0'), tensor(1090., device='cuda:0'), tensor(868., device='cuda:0'), tensor(1509., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[403.9349184, 739.5483648, 599.9501376, 1062.3968256]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0001a82e-14af-4db8-bfec-8b09f67612d2.jpg\n",
      "image loaded, shape: (1920, 1434, 3)\n",
      "human detected, bbox: [tensor(236., device='cuda:0'), tensor(579., device='cuda:0'), tensor(1106., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[449.86115814, 771.1683840000001, 657.64382898, 1055.2778495999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:04, 19.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 005c9c2f-1c90-4355-9b63-870680f23809.jpg\n",
      "image loaded, shape: (960, 1280, 3)\n",
      "human detected, bbox: [tensor(450., device='cuda:0'), tensor(127., device='cuda:0'), tensor(694., device='cuda:0'), tensor(882., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[548.8754944, 271.7381952, 600.9015039999999, 344.2490016]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0015865c-be5c-4966-9c49-1a1f2e511ffb.jpg\n",
      "image loaded, shape: (1920, 1434, 3)\n",
      "human detected, bbox: [tensor(23., device='cuda:0'), tensor(628., device='cuda:0'), tensor(1028., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[444.45483474, 1056.0956351999998, 627.91137336, 1345.0950527999998]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00a9cb1b-ce93-4953-a6ec-5a666ff5f1e5.jpg\n",
      "image loaded, shape: (1920, 1437, 3)\n",
      "human detected, bbox: [tensor(166., device='cuda:0'), tensor(428., device='cuda:0'), tensor(1302., device='cuda:0'), tensor(1916., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[735.72951504, 919.7629440000001, 944.64560454, 1191.7592064]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0041be71-ce6b-47e0-9686-8e6e56c5e0e5.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(16., device='cuda:0'), tensor(14., device='cuda:0'), tensor(1432., device='cuda:0'), tensor(1916., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[368.1289872, 1215.5415936, 777.843576, 1786.5935999999997]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [00:04, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "2 persons, \n",
      "Image 00bc44f3-0284-4a6a-892f-2131f68faf3d.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(305., device='cuda:0'), tensor(368., device='cuda:0'), tensor(1350., device='cuda:0'), tensor(1438., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[727.2115584000001, 813.7504224, 894.9230592000001, 1060.7527008], [893.8959936, 482.3967456, 1093.0324416, 704.4994944]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 001fccec-f203-4a22-8dfb-4f44d7b9a3a1.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(27., device='cuda:0'), tensor(202., device='cuda:0'), tensor(1334., device='cuda:0'), tensor(1908., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[326.143368, 693.6516672, 574.3526544, 1085.5747392]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 007df56b-f02c-437b-899a-1b2723e2987c.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(283., device='cuda:0'), tensor(713., device='cuda:0'), tensor(1236., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[473.26911839999997, 878.916288, 650.904408, 1132.4575488]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00bdd46e-ecd7-453a-aa49-c0a42aa78ed7.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(534., device='cuda:0'), tensor(171., device='cuda:0'), tensor(1416., device='cuda:0'), tensor(1432., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[516.724224, 1070.1945648, 670.8821376000001, 1309.0044528], [1230.0728448, 433.67188319999997, 1394.9850432, 668.5773408]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:04, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 006864d0-7f14-4a22-8f98-e949c1bd6166.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(156., device='cuda:0'), tensor(540., device='cuda:0'), tensor(1437., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[932.2145280000001, 1111.16016, 1287.6542496, 1703.7303936]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 009b4a76-9223-4801-a7dc-31d45cab3df9.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(202., device='cuda:0'), tensor(850., device='cuda:0'), tensor(880., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[432.58255199999996, 1246.222848, 572.2641072, 1458.0861696000002]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00c4bf83-2adb-42dc-8a22-e126a731cba3.jpg\n",
      "image loaded, shape: (1920, 1434, 3)\n",
      "human detected, bbox: [tensor(405., device='cuda:0'), tensor(910., device='cuda:0'), tensor(979., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[415.41640644, 1564.1623296, 509.43287369999996, 1697.514048], [750.05261016, 1024.280928, 849.1609963199999, 1149.6349632000001]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00cf039d-fe5f-49cb-b158-aa437f6c568a.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(10., device='cuda:0'), tensor(154., device='cuda:0'), tensor(1824., device='cuda:0'), tensor(1440., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[1031.2944, 726.1093152, 1412.7126528, 1267.334424]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:04, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00155881-1a8f-4541-9ad8-8323675d8c46.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(321., device='cuda:0'), tensor(748., device='cuda:0'), tensor(1352., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[810.5732208000001, 1251.2231616, 1020.8800656000001, 1585.3124352]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00818540-8153-41d3-aba7-2369eaea25b9.jpg\n",
      "image loaded, shape: (1080, 1920, 3)\n",
      "human detected, bbox: [tensor(564., device='cuda:0'), tensor(164., device='cuda:0'), tensor(1192., device='cuda:0'), tensor(1068., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[818.3738496000001, 441.114336, 962.1173952, 636.2727336]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00b0f3a6-37c7-48cd-9aa3-99ecdfbef18b.jpg\n",
      "image loaded, shape: (1920, 841, 3)\n",
      "human detected, bbox: [tensor(97., device='cuda:0'), tensor(591., device='cuda:0'), tensor(689., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[316.10803242000003, 907.032288, 449.76868384, 1120.2450623999998], [585.71390335, 1407.4035840000001, 677.7006836099999, 1572.995136]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00d88e7b-c676-463c-860e-c3afbb6d364a.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(111., device='cuda:0'), tensor(838., device='cuda:0'), tensor(1332., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[523.8842976, 1247.499264, 743.4260496, 1584.1929791999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:05, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 002d020f-7843-4018-a085-484e7319b00a.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(344., device='cuda:0'), tensor(446., device='cuda:0'), tensor(1478., device='cuda:0'), tensor(1438., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[703.9152, 869.0328, 924.3612096, 1213.0883999999999]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0020abdb-2c4e-4805-9b7f-b3a854d1e519.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(335., device='cuda:0'), tensor(119., device='cuda:0'), tensor(1075., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[615.1041216, 793.7803968000001, 802.6770384, 1020.9702144]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 00130511-f659-4015-b405-5f6d0d9f26c7.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(582., device='cuda:0'), tensor(538., device='cuda:0'), tensor(790., device='cuda:0'), tensor(1151., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[661.5069695999999, 664.4033472, 712.6642512, 726.3813504]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 00bea9e7-add1-4cc3-b4bc-bcf234966a77.jpg\n",
      "image loaded, shape: (1920, 1437, 3)\n",
      "human detected, bbox: [tensor(246., device='cuda:0'), tensor(124., device='cuda:0'), tensor(1101., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[492.28802043, 525.7641792000001, 651.98035032, 756.9708672], [1003.71982671, 1228.185792, 1094.86895226, 1478.4160895999999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:05, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 009d26fd-738a-4950-9ef3-8684dc02d446.jpg\n",
      "image loaded, shape: (960, 1280, 3)\n",
      "human detected, bbox: [tensor(468., device='cuda:0'), tensor(518., device='cuda:0'), tensor(869., device='cuda:0'), tensor(960., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[579.5610368, 676.9852992, 656.6782976000001, 794.7171648]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00b153d1-c817-41d4-a22d-ca08e9f221ab.jpg\n",
      "image loaded, shape: (1425, 1920, 3)\n",
      "human detected, bbox: [tensor(748., device='cuda:0'), tensor(566., device='cuda:0'), tensor(1193., device='cuda:0'), tensor(1421., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[882.332928, 820.4832224999999, 963.202272, 934.4227739999999]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00985f6d-aab5-49c6-a2de-e39d6045bce0.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(192., device='cuda:0'), tensor(365., device='cuda:0'), tensor(1114., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[609.0340896, 987.6870912, 784.1279088000001, 1261.0631232]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00835a5e-2f2d-46de-91fd-69db342969f9.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(87., device='cuda:0'), tensor(282., device='cuda:0'), tensor(1101., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[418.50711359999997, 857.733504, 632.4684047999999, 1115.7276864]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:05, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 0007edf9-a94e-46f7-98a4-a0fd6ba59654.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(341., device='cuda:0'), tensor(454., device='cuda:0'), tensor(1090., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[527.5002528, 754.3153536000001, 689.4203904, 970.2878784], [951.3349775999999, 1375.9332096, 1100.09916, 1601.7481728]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00ae4da4-9462-4804-ab99-5d2e409cd4b2.jpg\n",
      "image loaded, shape: (1920, 1434, 3)\n",
      "human detected, bbox: [tensor(0., device='cuda:0'), tensor(717., device='cuda:0'), tensor(1401., device='cuda:0'), tensor(1918., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[803.10974976, 1068.4888128, 1065.92350422, 1409.6502719999999]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 005258eb-43a0-4339-952e-ce60286f85a9.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(12., device='cuda:0'), tensor(368., device='cuda:0'), tensor(128., device='cuda:0'), tensor(517., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[709.0900992, 472.4892, 868.9543040000001, 692.694]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 005a5a71-a340-4032-9647-36d1ec1419f9.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(220., device='cuda:0'), tensor(608., device='cuda:0'), tensor(1434., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[426.2934096, 1048.121376, 652.6601856, 1385.5575168]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [00:05, 22.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 00b6c2e6-f283-4f7a-ab51-0e4e2da05b53.jpg\n",
      "image loaded, shape: (1920, 1080, 3)\n",
      "human detected, bbox: [tensor(252., device='cuda:0'), tensor(472., device='cuda:0'), tensor(914., device='cuda:0'), tensor(1914., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[440.61947999999995, 982.6191935999999, 595.5329628, 1190.515872]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 004881c0-533c-46f1-b7a8-e7633dd41f68.jpg\n",
      "image loaded, shape: (480, 640, 3)\n",
      "human detected, bbox: [tensor(11., device='cuda:0'), tensor(57., device='cuda:0'), tensor(585., device='cuda:0'), tensor(479., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[184.94428159999998, 212.64363360000002, 297.2277888, 380.8932048]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00aa414d-2b1e-4504-8ca8-b1da1ebb7538.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(633., device='cuda:0'), tensor(146., device='cuda:0'), tensor(1724., device='cuda:0'), tensor(1438., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[1089.1335552, 552.0026016, 1294.8857664, 871.6716144]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00cda552-8720-4165-b2a7-1b5e539ba8b2.jpg\n",
      "image loaded, shape: (1440, 1920, 3)\n",
      "human detected, bbox: [tensor(550., device='cuda:0'), tensor(616., device='cuda:0'), tensor(1296., device='cuda:0'), tensor(1440., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[760.8412992, 1324.2597408, 960.9749952, 1437.620544], [868.6351872, 843.5430576, 1024.9045056, 1039.2547824]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00b0135c-b2ed-439a-9bc1-8db448a0c451.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(36., device='cuda:0'), tensor(6., device='cuda:0'), tensor(1274., device='cuda:0'), tensor(720., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[903.0823808, 264.8463264, 1268.6056448, 718.0887312]]\n",
      "i 0\n",
      "2 persons, \n",
      "Image 00209a6a-a779-4af7-9307-b2a598406f43.jpg\n",
      "image loaded, shape: (720, 1280, 3)\n",
      "human detected, bbox: [tensor(183., device='cuda:0'), tensor(354., device='cuda:0'), tensor(344., device='cuda:0'), tensor(564., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[759.6414848000001, 331.45001279999997, 916.9226496, 563.1396624]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:05, 21.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 000484ab-5fd0-49b8-9253-23a22b71d7b1.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(282., device='cuda:0'), tensor(444., device='cuda:0'), tensor(1430., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[501.62999040000005, 1657.4670336, 875.1230064000001, 1906.5631872], [837.903888, 859.3820352, 1075.1846256000001, 1172.0039424]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 0030c39d-f220-4be5-9ae5-fe176c17a821.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(317., device='cuda:0'), tensor(530., device='cuda:0'), tensor(1420., device='cuda:0'), tensor(1916., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[756.5215968, 836.610816, 936.5750495999999, 1107.1706304]]\n",
      "i 0\n",
      "1 person, \n",
      "Image 00d56515-03a6-45a9-853e-5ae7ea46e374.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(206., device='cuda:0'), tensor(508., device='cuda:0'), tensor(1108., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[360.3227184, 877.6138559999999, 579.7249632, 1173.6594048]]\n",
      "i 0\n",
      "3 persons, \n",
      "Image 00a39254-1e82-4c34-823e-c1e1e25da52c.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(132., device='cuda:0'), tensor(778., device='cuda:0'), tensor(1144., device='cuda:0'), tensor(1920., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[342.8824032, 1077.7616064, 530.2064303999999, 1345.9166016]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:06, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "1 person, \n",
      "Image 001c5c43-c9fe-4766-b791-d6bf309c7f95.jpg\n",
      "image loaded, shape: (1920, 1440, 3)\n",
      "human detected, bbox: [tensor(19., device='cuda:0'), tensor(441., device='cuda:0'), tensor(1401., device='cuda:0'), tensor(1917., device='cuda:0')]\n",
      "ground truth gestures loaded, bboxes: [[461.4044688, 1131.9921408, 769.1532768, 1596.0994176]]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    weights=['./../training/runs/train/MIAP_person_detection/weights/best.pt'],\n",
    "    source=\"./../datasets/HaGRID_test/val/like.json\",\n",
    "    img_size=640,\n",
    "    conf_thres=0.317,\n",
    "    iou_thres=0.45,\n",
    "    device='0',\n",
    "    view_img=False,\n",
    "    save_txt=False,\n",
    "    save_conf=False,\n",
    "    nosave=False,\n",
    "    classes=None,\n",
    "    agnostic_nms=False,\n",
    "    augment=False,\n",
    "    update=False,\n",
    "    project='runs/detect',\n",
    "    name='reducing_detect_func',\n",
    "    exist_ok=False,\n",
    "    no_trace=False\n",
    ")\n",
    "\n",
    "detect(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a96ab-d2cf-4faf-8902-709024fc35af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314452a-2d77-405e-a5d1-2485fc63f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import xyxy2xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49957f-4ab2-4683-9170-c4ff2e416bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8013ae-2bbe-42af-b0a0-26f5bf65ce9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prep HaGRID_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "687cbf89-d484-466f-903c-de22ce5d1b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset_path = './../datasets/HaGRID/'\n",
    "test_dataset_path = './../datasets/HaGRID_test/'\n",
    "\n",
    "# Load the 100 test images\n",
    "test_images = [f for f in os.listdir(os.path.join(test_dataset_path, 'like')) if f.endswith('.jpg')]\n",
    "\n",
    "# Create a set for easier matching\n",
    "test_image_set = set(os.path.splitext(f)[0] for f in test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78bfaf55-ea54-4d53-a5d8-239002865ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11ced842-5eb2-4891-bc5e-793554dc6b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_image_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57b156de-246e-4f8b-ab46-42eb93f5081e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_image_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "241c923c-3bac-4120-bfab-133ad9518084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_dataset_path = './../datasets/HaGRID/'\n",
    "test_dataset_path = './../datasets/HaGRID_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34a739bc-5182-4854-8d95-ded342ee192d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list for the labels\n",
    "matched_labels = {}\n",
    "\n",
    "# Iterate through label files in the original dataset\n",
    "for split in ['train', 'val', 'test']:\n",
    "    label_path = os.path.join(original_dataset_path, split)\n",
    "    with open(os.path.join(label_path, \"like.json\"), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Check if images in the label file are in the test set\n",
    "        for image_id, info in data.items():\n",
    "            if image_id in test_image_set:\n",
    "                matched_labels[image_id] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efbf93ee-d34f-4da3-833b-6a491fbf7dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{002d020f-7843-4018-a085-484e7319b00a : {'bboxes': [[0.3666225, 0.603495, 0.11481563, 0.2389275]], 'user_id': '25bc760d508911e2c3453c3dd593092a0bf1b83c01817182eea3899fa76964c2', 'labels': ['like']}}\n",
      "{00818540-8153-41d3-aba7-2369eaea25b9 : {'bboxes': [[0.42623638, 0.4084392, 0.07486643, 0.18070222]], 'user_id': 'a1b0bb3f79eb35269ae7e804eea8557fb1c7e97259cb8b5d98bba7fc870d91da', 'labels': ['like']}}\n",
      "{00bf711f-e2cc-4cb5-b0ac-573198cd6455 : {'bboxes': [[0.32369747, 0.54091418, 0.11421507, 0.12234949], [0.59147623, 0.87560417, 0.13760348, 0.09844989]], 'user_id': '22ce972ece766dc47a240644048fb8399e4e69075f225c634bfaacb2a84e7423', 'labels': ['like', 'no_gesture']}}\n",
      "{0069d29e-4e71-433b-96e1-09657ca3d7eb : {'bboxes': [[0.42199823, 0.3240404, 0.08642329, 0.22055762]], 'user_id': '31ef5349bc0bdc95cb2dcede7fc6bdce4c99b4ea4e70bb495f50082ec1f32afe', 'labels': ['like']}}\n",
      "{009b4a76-9223-4801-a7dc-31d45cab3df9 : {'bboxes': [[0.30040455, 0.6490744, 0.09700108, 0.11034548]], 'user_id': 'dd7f74b0a40f86ca9348bb019ba2286d0f88ee08efa430592de2f791537f20cd', 'labels': ['like']}}\n",
      "{0035dfca-3c94-49f4-93fb-df03dc1514ae : {'bboxes': [[0.38868563, 0.64242464, 0.07534302, 0.05884204], [0.46138667, 0.50762531, 0.07004173, 0.06356569]], 'user_id': '840d804f1c3c1462c2cb976ef10082e7b0ac3d49c4170ae4761d8e314530dc66', 'labels': ['no_gesture', 'like']}}\n",
      "{00b153d1-c817-41d4-a22d-ca08e9f221ab : {'bboxes': [[0.4595484, 0.5757777, 0.04211945, 0.07995758]], 'user_id': '6839b0e20d092baf9bb7069f718b203efa8c70e516993af69118141ce2ad61bc', 'labels': ['like']}}\n",
      "{00aa414d-2b1e-4504-8ca8-b1da1ebb7538 : {'bboxes': [[0.56725706, 0.38333514, 0.10716261, 0.22199237]], 'user_id': 'e8f04d01854f5280f74058ae0bab7ecc5cb3b266798d1e21ee1d49b94829a9f2', 'labels': ['like']}}\n",
      "{0022cb1a-e7c1-48aa-b48c-027dca084de9 : {'bboxes': [[0.50266014, 0.58770121, 0.04939934, 0.08882181]], 'user_id': 'bacd8dcfe4518894842cc4d6c4d0e1d27063e80fe15c6f5f8b3c4099cfba6083', 'labels': ['like']}}\n",
      "{0036c112-1ede-4c28-9096-74e050a5bf95 : {'bboxes': [[0.47325649, 0.47679759, 0.09367701, 0.124734]], 'user_id': '6a96376d360d3046540d6ff098b9e631d92936b74558a1927aa99dbae9acc78e', 'labels': ['like']}}\n"
     ]
    }
   ],
   "source": [
    "sample_keys=random.sample(list(matched_labels.keys()), min(10, len(matched_labels)))\n",
    "for image_id in sample_keys:\n",
    "    print(f'{{{image_id} : {matched_labels[image_id]}}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ab525e5-88d1-44ad-937e-ff617b6e2736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matched labels saved to ./../datasets/HaGRID_test/val/like.json.\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(test_dataset_path, 'val', 'like.json')\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(matched_labels, outfile, indent=4)\n",
    "\n",
    "print(f\"\\nMatched labels saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcae8adf-3ee4-4f90-a913-7cf058d29422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for image_id, info in data.items():\n",
    "        test_labels.append(image_id)\n",
    "\n",
    "test_label_set = set(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67e90ee0-85e2-4f83-9298-e243903ef148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e8dda51-15d4-4c18-9e15-06aee6f58d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9d12424-d604-4167-b401-e19858623447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_set == test_label_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo)",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
