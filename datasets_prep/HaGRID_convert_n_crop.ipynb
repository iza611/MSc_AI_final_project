{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4237218d-65c4-4739-941f-15a33e37a239",
   "metadata": {
    "tags": []
   },
   "source": [
    "loading imgs into LoadImages class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd2a00-0594-4cf6-9e4d-b98c74a1ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on yolo/utils/datasets.py\n",
    "class LoadImages():\n",
    "    def __init__(self, json_path, img_size=640, stride=32):\n",
    "        # load all labels from given dir as a list\n",
    "        imgs = []\n",
    "        for x in y: # go trough each label in a list\n",
    "            # given label xyz123 set img path to './../datasets/HaGRID/like/xyz123.jpg\n",
    "            # load img from path \n",
    "        \n",
    "        # x1,y1,x2,y2 = hagrid_to_xyxy()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.files = images + videos\n",
    "        self.nf = ni + nv  # number of files\n",
    "        self.video_flag = [False] * ni + [True] * nv\n",
    "        self.mode = 'image'\n",
    "        if any(videos):\n",
    "            self.new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
    "                            f'Supported formats are:\\nimages: {img_formats}\\nvideos: {vid_formats}'\n",
    "            \n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.count == self.nf:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093c8c8-00cb-45cc-9728-d5010578aeea",
   "metadata": {},
   "source": [
    "the detect code copied and modified from the yolov7 repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf8233-7c15-4632-a6f4-017154068ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "# for the human bbx\n",
    "# ensure only 1 bbx with highest conf\n",
    "# if zero then set xyxy to img dimensions\n",
    "\n",
    "given json_path\n",
    "get all gestures bbxs and convert to xyxy and make a gestures bbx dict\n",
    "\n",
    "then do the slightly modified detect()\n",
    "during this iteration trough human bbx dict, gestures bbx dict and LoadImages, create dictionary of dictionaries\n",
    "like this\n",
    "\n",
    "# for filename in human_bbx.keys():  # Assuming all dictionaries have the same keys\n",
    "    combined_dict[filename] = {\n",
    "        'human_bbx': just detected human_bbx,\n",
    "        'gestures_bbxs': gestures_bbxs.get(filename),\n",
    "        'image': img loaded from LoadImages\n",
    "    }\n",
    "    \n",
    "at the end return dict\n",
    "\n",
    "clean memory to remove LoadImages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286d017-975d-430a-ad33-59ab781b4a2b",
   "metadata": {},
   "source": [
    "crop with albumentations, convert & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd1450-a479-489a-b31c-96ef7664927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy_to_xywh():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc43d0-fcc5-4620-b828-93ae40dce9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "# Define the transformation\n",
    "def crop_n_transform(image_path, human_bbox, hand_bboxes):\n",
    "    x1, y1, x2, y2 = human_bbox\n",
    "    transform = A.Compose([\n",
    "        A.Crop(x_min=x1, y_min=y1, x_max=x2, y_max=y2, p=1.0),\n",
    "    ])\n",
    "    \n",
    "    # Convert bounding boxes to the required format\n",
    "    bboxes = [[x1, y1, x2, y2] for (hx1, hy1, hx2, hy2) in hand_bboxes]\n",
    "    \n",
    "    # load image from path\n",
    "    \n",
    "    # Apply the transformation\n",
    "    transformed = transform(image=image, bboxes=bboxes)\n",
    "    return transformed['image'], transformed['bboxes']\n",
    "\n",
    "for filename, data in combined_dict.items():\n",
    "    human_bbx = data['human_bbx']\n",
    "    gestures_bbxs = data['gestures_bbxs']\n",
    "    image = data['image']\n",
    "    \n",
    "    crop_and_transform()\n",
    "    xyxy_to_xywh()\n",
    "    save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f55447-279d-41e6-9010-3202d19c7c00",
   "metadata": {},
   "source": [
    "check if all the imgs in HaGRID match labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e342ba-5dd8-417c-b949-e3a95ea2141b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fb0bdf-4aa9-42ce-bc25-6e9550ff266e",
   "metadata": {},
   "source": [
    "ps. probably best to split train jsons into parts (eg like_p1 and like_p2); but first start with val, then test, then train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo)",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
